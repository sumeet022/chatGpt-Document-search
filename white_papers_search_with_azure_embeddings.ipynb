{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sudarshan-koirala/youtube-stuffs/blob/main/langchain/Chat_with_Any_Documents_Own_ChatGPT_with_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.179\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Load environment variables (set OPENAI_API_KEY and OPENAI_API_BASE in .env)\n",
    "load_dotenv()\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "openai.api_base =  os.getenv('OPENAI_API_BASE')\n",
    "openai.api_key = os.getenv(\"AZ_OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize gpt-35-turbo and our embedding model\n",
    "llm = AzureChatOpenAI(deployment_name=\"cresen-gpt-35-turbo\", openai_api_version=\"2023-03-15-preview\") # For Chat\n",
    "embeddings = OpenAIEmbeddings(model = 'text-embedding-ada-002', deployment='text-embedding-ada-002',chunk_size=1) # For Embeddings\n",
    "#embeddings = OpenAIEmbeddings(deployment=\"cresen-embedding\",model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Documents Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 3 document(s) in your data\n",
      "There are a total of 7624 characters in your documents\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "pdf_loader = DirectoryLoader(r\"C:\\Users\\Ananthu Raj C M\\Documents\\Python\\ChatGPT_Trials\\ChatGPT-Tabular-Data-main\\Files\", glob=\"**/*.pdf\")\n",
    "#take all the loader\n",
    "loaders = [pdf_loader] # , readme_loader, txt_loader\n",
    "#lets create document \n",
    "documents = []\n",
    "for loader in loaders:\n",
    "    documents.extend(loader.load())\n",
    "print (f'You have {len(documents)} document(s) in your data')\n",
    "print (f'There are a total of {len(documents[0].page_content)} characters in your documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Splitting Text from the documents and creating Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=20) #chunk overlap seems to work better\n",
    "#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=40) \n",
    "documents = text_splitter.split_documents(documents)\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"666 Exton Commons Exton PA 19341 (484) 879-426\\n\\nTaking the mystery and risk out of your Healthcare Engagements with EngageMate by Cresen Solutions\\n\\nCresen Solutions was formed to provide distinct and qualified services to the life sciences industry.\\n\\nOur organization has a clear focus and mission to enable improved business outcomes through advanced technology solutions and analytical expertise. The leadership team is comprised of seasoned healthcare industry experts focused on enabling the technological advancements necessary in today's ever-changing healthcare landscape. Strong foundations in technology systems and solutions and the business acumen to enable predictable business results are keys to our success. Experience, quality, and innovation are the cornerstones of our team's capabilities and are applied to each engagement.\\n\\nEngageMate© is an HCP/HCO engagements tool developed by Cresen Solutions that enables our\\n\\nclients to fully track HCP and HCO engagements throughout the entire life cycle. The system is designed to automate the business lifecycle of any fee for service engagement from business planning through close- out. It will also serve as the auditable system of record for all associated HCP/HCO fee-based contractual arrangements. There are many problems to be solved in the engagement lifecycle, EngageMate shines in the way it solves these problems.\\n\\nBelow in figure 1 is a diagram of the entire lifecycle that is commonly utilized within the Life\\n\\nSciences Industry when fee for service contractual services are required:\\n\\n666 Exton Commons Exton PA 19341 (484) 879-426\\n\\nFigure 1: HCP Engagement Lifecycle\\n\\nAll life sciences companies hire health care professionals (HCPs), organizations (HCOs), and other\\n\\nservice providers. Engaging a service provider is often a lengthy process with many moving pieces and interactions between departments. During that time, the Compliance department is attempting to keep track of, and approve, all activities and service providers to ensure compliance with external government regulations and internal company policies. The HCP engagement process involves workflow including fair market value (FMV) assessments and exceptions, activity concept reviews, nomination review and approvals, content review/approvals, monetary spend capture, and appropriate certification and activity close-out. As an added layer of complexity, the events can be held around the globe with service providers from many countries, all of whom need their own respective cross-border workflows. While it is possible to track the engagement process via spreadsheets, word documents, and emails, this can be messy and error prone. In today's age of big data and required government reporting, the difficult task of manually tracking engagements is quickly becoming impossible. Now, more than ever, it is essential to have an easy- to-use system where a company can track and manage activities, approvals, and FMV tiers and rates all in one place.\\n\\nEngageMate© is an elegant system that solves all the problems associated with the engagement\\n\\nlifecycle. Recently, after an unsatisfactory experience with another engagement tool, a top pharmaceutical company returned to tracking their activities with emails, spreadsheets, and word documents. Cresen Solutions was then brought in to implement EngageMate©. After gathering business requirements around workflow requirements, business rules, and input forms EngageMate© was configured to meet all of the client’s policies and SOPs. EngageMate© was integrated with their customer\\n\\n666 Exton Commons Exton PA 19341 (484) 879-426\\n\\nmaster to pull service provider information into the system, ensuring accurate Customer(HCP or HCO) details while limiting manual data entry. Review workflows were created for each region within the company enabling a smooth process for the activity to be reviewed by multiple departments and reviewers across the globe. After the workflows are completed EngageMate© assures proper closure and certification processes aligned with company policy. EngageMate© uses all the latest technologies and uses common open integration methodologies to enable easy interfacing with other company business systems.\\n\\nFrom adding service providers to filling out forms and submitting activities, business users have\\n\\nenjoyed EngageMate’s© responsiveness and ease of use. Most users were able to pick up the navigation and use of the system after one self-guided training session. Everyone, both “Submitters” and “Reviewers”, receive email notifications when an activity is ready for their action. The email provides an embedded hyperlink that takes users directly to the activity they need to take action on. Additionally, upon logging into EngageMate©, users are presented with a simple landing page showing tiles that separate activities into distinct categories allowing users to see a filtered list of activities to find any activity they are looking for expeditiously. It also includes customizable dashboards that pull\", metadata={'source': 'C:\\\\Users\\\\Ananthu Raj C M\\\\Documents\\\\Python\\\\ChatGPT_Trials\\\\ChatGPT-Tabular-Data-main\\\\Files\\\\EngageMate Whitepaper.pdf'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding with Azure Embedding Model and storing it in ChromaDB Vectorestore\n",
    "\n",
    "Langchain and ChromaDB : https://blog.langchain.dev/langchain-chroma/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: db\n"
     ]
    }
   ],
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'db'\n",
    "vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=persist_directory)\n",
    "vectorstore.persist()\n",
    "vectorstore = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from langchain.vectorstores import FAISS\n",
    "#db = FAISS.from_documents(documents=documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'as_retriever'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44948\\3079503596.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mretriever\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_retriever\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"similarity\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"k\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mqa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConversationalRetrievalChain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_llm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOpenAI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretriever\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'as_retriever'"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Adapt if needed\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\")\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "                                           retriever=vectorstore.as_retriever(),\n",
    "                                           condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "                                           return_source_documents=True,\n",
    "                                           verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is Spendmate?\n",
      "Answer: SpendMate is a healthcare compliance transparency tool designed to streamline the end-to-end business process of transparency reporting. It enables companies to meet the growing demand for transparency between industry and medical providers in the form of report preparation, generation, and submission, doing so cost-effectively, and it also easily scales with companies as they expand globally. SpendMate simplifies the spend reporting process by providing the flexibility and scalability needed by clients to meet the compliance demands of external entities.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"what is Spendmate?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which products does cresen solutions have?\n",
      "Answer: Cresen Solutions offers multiple products including EngageMate, MonitorMate, and SpendMate. They also offer a free PowerCMS tool for analyzing Open Payments data.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"Which products does cresen solutions have?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43Z6U7XNMl0A"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6k29lnn-hF7"
   },
   "source": [
    "%%capture\n",
    "!pip install openai langchain  tiktoken pypdf unstructured[local-inference] gradio chromadb\n",
    "!pip install watermark\n",
    "!pip install chromadb\n",
    "pip install unstructured\n",
    "pip install pdf2image\n",
    "pip install pytesseract\n",
    "pip install detectron2\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TKuV8k5wDhpf",
    "outputId": "f30267e4-bc3c-42a4-c2e9-b7f69f396d7c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Ananthu Raj C M\" -vmp langchain,openai,chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EL9SFDqQ_F-O"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHqY2lzsL-Dj"
   },
   "source": [
    "### Using Chroma for storing vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'db'\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents=documents, embedding=embedding, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.persist()\n",
    "vectorstore = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying from ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7zOKdK4shnzc",
    "outputId": "2ed81422-bc9a-4816-a337-556d675fa204"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: db\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(documents, embeddings,persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: db\n"
     ]
    }
   ],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dbUFZWoXJ3d2"
   },
   "outputs": [],
   "source": [
    "query = \"What is spendmate ?\"\n",
    "docs = vectorstore.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSqHUoQRNmCI",
    "outputId": "3fedd512-9c1e-4f43-d493-beda1d8a49e2"
   },
   "outputs": [],
   "source": [
    "len(docs) #it went on and search on the 4 different vectors to find the similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YmSZQsDENfiE",
    "outputId": "37363217-3098-4f2c-c646-8d991502abf5"
   },
   "outputs": [],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nyoXLl0YUqz1",
    "outputId": "d2af77a9-e59a-4d74-fe4a-da51e95ec8e1"
   },
   "outputs": [],
   "source": [
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etsPkZROVpo3"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keEibikCN6-w"
   },
   "source": [
    "## Now the langchain part (Chaining with Chat History) \n",
    "- There are many chains but we use this [link](https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IiSQ4IQCtDT8"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_ZGMwOuaNiHZ"
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1TnRF83PMZV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "LxpEcvZYoN3D",
    "outputId": "9b0c3539-9e1d-49f6-8e69-c8798680e391",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "query = \"How is spendmate?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3QZzJ0D-6QY",
    "outputId": "67519e6c-488c-4948-d992-10236fab0a3a"
   },
   "outputs": [],
   "source": [
    "chat_history.append((query, result[\"answer\"]))\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "wtn22jDK_6Qj",
    "outputId": "eb65e3e9-b1a4-4680-c8b6-e7b215b4cc7a"
   },
   "outputs": [],
   "source": [
    "query = \"What is this number multiplied by 2?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5oduK4NVv1e"
   },
   "source": [
    "## Create a chatbot with memory with  widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vYMWTZoxV1Rq"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214,
     "referenced_widgets": [
      "93bdfcd316b94785a9f63b54fb2e7eed",
      "194fcb196a394e8cb78d9e39ef9461c0",
      "87f711d699b443eeac5301e31e139210",
      "5434caeb5dc840a0a1996b7f1151e9d1",
      "52fddac56425425fb1b3f6c41a83ad40",
      "6119c20941e645fe85777d8a40134f45",
      "7ebbefa2c67d44a6bb7a1ceaf61f9b63",
      "da35600790c34a9db211e75bbd47d00b",
      "0a59ae49aefe454dac82fd3cfa509201",
      "9c92fcac4b2b44dabe4210335e479af6",
      "d30bddef5eff4ba39adbfc977d1b543d",
      "3f22048167ab4e6a99ff352e4881d6f5",
      "8217b151f57a44d68f2135fba059de9f",
      "96f4fd5d75874fa3a9ba1f6d74b86855",
      "cb41fc15e4e84ddea00a1a63544486b6"
     ]
    },
    "id": "Q88JFgfLV9hQ",
    "outputId": "b262ee30-8863-46b5-e6e0-7160c6f17bf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with your data. Type 'exit' to stop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb8cec28f9d4568b0ec1d77e7b7ff07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Please enter your question:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98de8cf20f440feb286770be441c49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> What is spendmate ?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64842f142d9e49cbb5c0aa89e52cb982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"Orange\">Chatbot:</font></b> SpendMate is a healthcare compliance transparency tool…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058fd7cab1934db8ba6b259309b2592c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> What are the products available at cresen solutions ?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84df1a0bc024b47ad0ae14d78db272a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"Orange\">Chatbot:</font></b> Cresen Solutions offers three flagship products: Engag…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "def on_submit(_):\n",
    "    query = input_box.value\n",
    "    input_box.value = \"\"\n",
    "    \n",
    "    if query.lower() == 'exit':\n",
    "        print(\"Thanks for the chat!\")\n",
    "        return\n",
    "    \n",
    "    result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "    chat_history.append((query, result['answer']))\n",
    "    \n",
    "    display(widgets.HTML(f'<b>User:</b> {query}'))\n",
    "    display(widgets.HTML(f'<b><font color=\"Orange\">Chatbot:</font></b> {result[\"answer\"]}'))\n",
    "\n",
    "print(\"Chat with your data. Type 'exit' to stop\")\n",
    "\n",
    "input_box = widgets.Text(placeholder='Please enter your question:')\n",
    "input_box.on_submit(on_submit)\n",
    "\n",
    "display(input_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rVF__4XPdwY"
   },
   "source": [
    "## Gradio Part (Building the [chatbot like UI](https://gradio.app/docs/#chatbot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ0kvK0tOt_O"
   },
   "source": [
    "### Gradio sample example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "id": "VoKW01NoOsml",
    "outputId": "e6719dab-8e98-4107-9522-d5cb482caddc"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def respond(message, chat_history):\n",
    "        print(message)\n",
    "        print(chat_history)\n",
    "        bot_message = random.choice([\"How are you?\", \"I love you\", \"I'm very hungry\"])\n",
    "        chat_history.append((message, bot_message))\n",
    "        print(chat_history)\n",
    "        return \"\", chat_history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wDesKT8Oz8x"
   },
   "source": [
    "### Gradio langchain example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ug2RL9rdPXa4",
    "outputId": "095d2936-a255-4e17-8262-60014471c474"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "    \n",
    "    def respond(user_message, chat_history):\n",
    "        print(user_message)\n",
    "        print(chat_history)\n",
    "        # Get response from QA chain\n",
    "        response = qa({\"question\": user_message, \"chat_history\": chat_history})\n",
    "        # Append user message and response to chat history\n",
    "        chat_history.append((user_message, response[\"answer\"]))\n",
    "        print(chat_history)\n",
    "        return \"\", chat_history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot], queue=False)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9V1nvnA__OA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPZw2K7M/AjT+BeF+DKgYK4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a59ae49aefe454dac82fd3cfa509201": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "194fcb196a394e8cb78d9e39ef9461c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f22048167ab4e6a99ff352e4881d6f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "52fddac56425425fb1b3f6c41a83ad40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5434caeb5dc840a0a1996b7f1151e9d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52fddac56425425fb1b3f6c41a83ad40",
      "placeholder": "​",
      "style": "IPY_MODEL_6119c20941e645fe85777d8a40134f45",
      "value": "<b>User:</b> who are the authors of gpt4al"
     }
    },
    "6119c20941e645fe85777d8a40134f45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ebbefa2c67d44a6bb7a1ceaf61f9b63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da35600790c34a9db211e75bbd47d00b",
      "placeholder": "​",
      "style": "IPY_MODEL_0a59ae49aefe454dac82fd3cfa509201",
      "value": "<b><font color=\"Orange\">Chatbot:</font></b>  The authors of GPT4All are Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin M. Schmidt, Adam Treat, and Andriy Mulyar."
     }
    },
    "8217b151f57a44d68f2135fba059de9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96f4fd5d75874fa3a9ba1f6d74b86855",
      "placeholder": "​",
      "style": "IPY_MODEL_cb41fc15e4e84ddea00a1a63544486b6",
      "value": "<b><font color=\"Orange\">Chatbot:</font></b> \n\nPandas AI is a Python library that adds generative artificial intelligence capabilities to Pandas, the popular data analysis and manipulation tool. It is designed to be used in conjunction with Pandas, and is not a replacement for it."
     }
    },
    "87f711d699b443eeac5301e31e139210": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93bdfcd316b94785a9f63b54fb2e7eed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "TextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextView",
      "continuous_update": true,
      "description": "",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_194fcb196a394e8cb78d9e39ef9461c0",
      "placeholder": "Please enter your question:",
      "style": "IPY_MODEL_87f711d699b443eeac5301e31e139210",
      "value": ""
     }
    },
    "96f4fd5d75874fa3a9ba1f6d74b86855": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c92fcac4b2b44dabe4210335e479af6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d30bddef5eff4ba39adbfc977d1b543d",
      "placeholder": "​",
      "style": "IPY_MODEL_3f22048167ab4e6a99ff352e4881d6f5",
      "value": "<b>User:</b> what is pandas ai "
     }
    },
    "cb41fc15e4e84ddea00a1a63544486b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d30bddef5eff4ba39adbfc977d1b543d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da35600790c34a9db211e75bbd47d00b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
